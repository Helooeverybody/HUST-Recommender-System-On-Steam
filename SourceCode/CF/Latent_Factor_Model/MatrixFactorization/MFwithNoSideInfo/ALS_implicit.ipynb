{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from math import log2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from cmfrec import CMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"../preprocessed_data/ratings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3483739, 4)\n",
      "(870400, 4)\n"
     ]
    }
   ],
   "source": [
    "rec_train, rec_test = train_test_split(ratings,test_size= 0.2, random_state= 42)\n",
    "users_train,games_train = rec_train[\"user_id\"].unique(), rec_train[\"app_id\"].unique()\n",
    "rec_test =rec_test.loc[rec_test[\"user_id\"].isin(users_train) & rec_test[\"app_id\"].isin(games_train)]\n",
    "print(rec_train.shape)\n",
    "print(rec_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recode function for implicit feedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ratings(predictions):\n",
    "    actual = np.array([pred[2] for pred in predictions])\n",
    "    pred = np.array([pred[3] for pred in predictions])\n",
    "    return actual, pred\n",
    "\n",
    "def get_errors(predictions):\n",
    "    actual, pred = get_ratings(predictions)\n",
    "    rmse = np.sqrt(np.mean((pred - actual)**2))\n",
    "    mape = np.mean(np.abs(actual - pred) / actual) * 100\n",
    "    return rmse, mape\n",
    "\n",
    "def evaluation(predictions, k=5):\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "    \n",
    "    precisions = defaultdict(float)\n",
    "    recalls = defaultdict(float)\n",
    "    ndcgs = defaultdict(float)\n",
    "    f1_scores = defaultdict(float)\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        dcg = sum((true_r / log2(i + 2)) for i, (_, true_r) in enumerate(user_ratings[:k]) if true_r == 1)\n",
    "        sorted_true = sorted(user_ratings, key=lambda x: x[1], reverse=True)\n",
    "        idcg = sum((true_r / log2(i + 2)) for i, (_, true_r) in enumerate(sorted_true[:k]) if true_r == 1)\n",
    "        \n",
    "        n_rel = sum((true_r == 1) for (_, true_r) in user_ratings)\n",
    "        n_rec_k = sum((est >= 0.5) for (est, _) in user_ratings[:10])\n",
    "        n_rel_and_rec_k = sum((true_r == 1 and est >= 0.5) for (est, true_r) in user_ratings[:10])\n",
    "        \n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "        ndcgs[uid] = dcg / idcg if idcg != 0 else 0\n",
    "        f1_scores[uid] = 2* precisions[uid] * recalls[uid] / (precisions[uid]+ recalls[uid]) if precisions[uid]+recalls[uid] !=0 else 0\n",
    "    \n",
    "    precision = np.mean(list(precisions.values()))\n",
    "    recall = np.mean(list(recalls.values()))\n",
    "    ndcg = np.mean(list(ndcgs.values()))\n",
    "    f1_score = sum(f1 for f1 in f1_scores.values()) / len(f1_scores)\n",
    "    \n",
    "    return precision, recall, f1_score, ndcg\n",
    "\n",
    "def run_implicit(algo, train_data, test_data, verbose=True, U=None, I=None):\n",
    "    start = datetime.now()\n",
    "    train = dict()\n",
    "    test = dict()\n",
    "    train_data = train_data.rename(columns={'app_id': 'ItemId', 'user_id': 'UserId', 'implicit_rating': 'Rating'})\n",
    "    test_data = test_data.rename(columns={'app_id': 'ItemId', 'user_id': 'UserId', 'implicit_rating': 'Rating'})\n",
    "    if U is not None:\n",
    "        U = U.rename(columns={'user_id': 'UserId'})\n",
    "    if I is not None:\n",
    "        I = I.rename(columns={'app_id': 'ItemId'})\n",
    "    \n",
    "    st = datetime.now()\n",
    "    print('Training the model...')\n",
    "    algo.fit(X=train_data, U=U, I=I)\n",
    "    print('Done. Time taken: {} \\n'.format(datetime.now() - st))\n",
    "\n",
    "    st = datetime.now()\n",
    "    print('Evaluating the model with train data...')\n",
    "    preds = algo.predict(train_data['UserId'].tolist(), train_data['ItemId'].tolist())\n",
    "    train_preds = [(u, i, r, pred) for u, i, r, pred in zip(train_data['UserId'], train_data['ItemId'], train_data['Rating'], preds)]\n",
    "    precision, recall, f1, ndcg = evaluation(train_preds)\n",
    "    print('Time taken: {}'.format(datetime.now() - st))\n",
    "    \n",
    "    if verbose:\n",
    "        print('-' * 15)\n",
    "        print('Train Data')\n",
    "        print('-' * 15)\n",
    "        print(\"Precision: {}\\nRecall: {}\\nF1: {}\\nNDCG: {}\\n\".format(precision, recall, f1, ndcg))\n",
    "    \n",
    "    if verbose:\n",
    "        print('Adding train results to the dictionary...')\n",
    "    train['precision'] = precision\n",
    "    train['recall'] = recall\n",
    "    train['f1'] = f1\n",
    "    train['ndcg'] = ndcg\n",
    "\n",
    "    st = datetime.now()\n",
    "    print('\\nEvaluating the model with test data...')\n",
    "    preds = algo.predict(test_data['UserId'].tolist(), test_data['ItemId'].tolist())\n",
    "    test_preds = [(u, i, r, pred) for u, i, r, pred in zip(test_data['UserId'], test_data['ItemId'], test_data['Rating'], preds)]\n",
    "    precision, recall, f1, ndcg = evaluation(test_preds)\n",
    "   \n",
    "    print('Time taken: {}'.format(datetime.now() - st))\n",
    "    if verbose:\n",
    "        print('-' * 15)\n",
    "        print('Test Data')\n",
    "        print('-' * 15)\n",
    "        print(\"Precision: {}\\nRecall: {}\\nF1: {}\\nNDCG: {}\\n\".format(precision, recall, f1, ndcg))\n",
    "    \n",
    "    if verbose:\n",
    "        print('Storing the test results in the test dictionary...')\n",
    "    test['precision'] = precision\n",
    "    test['recall'] = recall\n",
    "    test['f1'] = f1\n",
    "    test['ndcg'] = ndcg\n",
    "\n",
    "    print('\\n' + '-' * 45)\n",
    "    print('Total time taken to run this algorithm:', datetime.now() - start)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Done. Time taken: 0:00:18.767879 \n",
      "\n",
      "Evaluating the model with train data...\n",
      "Time taken: 0:00:22.770834\n",
      "---------------\n",
      "Train Data\n",
      "---------------\n",
      "Precision: 0.9342407070176895\n",
      "Recall: 0.24780574702330174\n",
      "F1: 0.3797450134129907\n",
      "NDCG: 0.9492527609130151\n",
      "\n",
      "Adding train results to the dictionary...\n",
      "\n",
      "Evaluating the model with test data...\n",
      "Time taken: 0:00:09.845453\n",
      "---------------\n",
      "Test Data\n",
      "---------------\n",
      "Precision: 0.8614880445841946\n",
      "Recall: 0.8020874606771221\n",
      "F1: 0.7980313378592722\n",
      "NDCG: 0.9234581719383568\n",
      "\n",
      "Storing the test results in the test dictionary...\n",
      "\n",
      "---------------------------------------------\n",
      "Total time taken to run this algorithm: 0:00:51.442951\n"
     ]
    }
   ],
   "source": [
    "als = CMF(k= 50 , method = 'als', lambda_ = 35)\n",
    "train, test = run_implicit(als , rec_train, rec_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
